{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Car Battery Failure And Model Explainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your goal in this notebook is to **predict how much time a car battery has left until it is expected to fail and use the automated machine learning model explainer to retrieve important features for predictions**. You are provided training data that includes telemetry from different vehicles, as well as the expected battery life that remains. From this you will train a model that given just the vehicle telemetry predicts the expected battery life. You will use automlexplainer module to retrieve feature importance for all iterations and then explain the model with previously unseen test data.\n",
    "\n",
    "You will use Azure Machine Learning SDK to **locally** train a **set** of models using **Automated Machine Learning**, evaluate performance of each model and pick the best performing model to retrive important features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "To begin, you will need to provide the following information about your Azure Subscription. \n",
    "\n",
    "In the following cell, be sure to set the values for `subscription_id`, `resource_group`, `workspace_name` and `workspace_region` as directed by the comments (*these values can be acquired from the Azure Portal*).\n",
    "\n",
    "**Be sure to replace XXXXX in the values below with your unique identifier.**\n",
    "\n",
    "To get these values, do the following:\n",
    "1. Navigate to the Azure Portal and login with the credentials provided.\n",
    "2. From the left hand menu, under Favorites, select `Resource Groups`.\n",
    "3. In the list, select the resource group with the name similar to `XXXXX`.\n",
    "4. From the Overview tab, capture the desired values.\n",
    "\n",
    "Execute the following cell by selecting the `>|Run` button in the command bar above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Provide the Subscription ID of your existing Azure subscription\n",
    "subscription_id = \"xxx-xxx-xxx\"\n",
    "\n",
    "#Provide values for the existing Resource Group \n",
    "resource_group = \"Interpretability-Lablet\"\n",
    "\n",
    "#Provide the Workspace Name and Azure Region of the Azure Machine Learning Workspace\n",
    "workspace_name = \"interpretability-workspace\"\n",
    "workspace_region = \"eastus\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'interpretability'\n",
    "project_folder = './interpretability'\n",
    "\n",
    "# this is the URL to the CSV file containing the training data\n",
    "data_url = \"https://databricksdemostore.blob.core.windows.net/data/connected-car/training-formatted.csv\"\n",
    "\n",
    "# this is the URL to the CSV file containing a small set of test data\n",
    "test_data_url = \"https://databricksdemostore.blob.core.windows.net/data/connected-car/fleet-formatted.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Azure Machine Learning SDK provides a comprehensive set of a capabilities that you can use directly within a notebook including:\n",
    "- Creating a **Workspace** that acts as the root object to organize all artifacts and resources used by Azure Machine Learning.\n",
    "- Creating **Experiments** in your Workspace that capture versions of the trained model along with any desired model performance telemetry. Each time you train a model and evaluate its results, you can capture that run (model and telemetry) within an Experiment.\n",
    "- Creating **Compute** resources that can be used to scale out model training, so that while your notebook may be running in a lightweight container in Azure Notebooks, your model training can actually occur on a powerful cluster that can provide large amounts of memory, CPU or GPU. \n",
    "- Using **Automated Machine Learning (AutoML)** to automatically train multiple versions of a model using a mix of different ways to prepare the data and different algorithms and hyperparameters (algorithm settings) in search of the model that performs best according to a performance metric that you specify. \n",
    "- Using **automlexplainer** module to retrive model explanation form existing run and explain model on new datasets.\n",
    "- Packaging a Docker **Image** that contains everything your trained model needs for scoring (prediction) in order to run as a web service.\n",
    "- Deploying your Image to either Azure Kubernetes or Azure Container Instances, effectively hosting the **Web Service**.\n",
    "\n",
    "In Azure Notebooks, all of the libraries needed for Azure Machine Learning are pre-installed. To use them, you just need to import them. Run the following cell to do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import azureml.core\n",
    "from azureml.core.experiment import Experiment\n",
    "from azureml.core.workspace import Workspace\n",
    "from azureml.core.model import Model\n",
    "from azureml.train.automl import AutoMLConfig\n",
    "from azureml.train.automl.run import AutoMLRun\n",
    "from azureml.train.automl.automlexplainer import retrieve_model_explanation\n",
    "from azureml.train.automl.automlexplainer import explain_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and connect to an Azure Machine Learning Workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to create a new Azure Machine Learning **Workspace** and save the configuration to disk (next to the Jupyter notebook). \n",
    "\n",
    "**Important Note**: You will be prompted to login in the text that is output below the cell. Be sure to navigate to the URL displayed and enter the code that is provided. Once you have entered the code, return to this notebook and wait for the output to read `Workspace configuration succeeded`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By using the exist_ok param, if the worskpace already exists you get a reference to the existing workspace\n",
    "# allowing you to re-run this cell multiple times as desired (which is fairly common in notebooks).\n",
    "ws = Workspace.create(\n",
    "    name = workspace_name,\n",
    "    subscription_id = subscription_id,\n",
    "    resource_group = resource_group, \n",
    "    location = workspace_region,\n",
    "    exist_ok = True)\n",
    "\n",
    "ws.write_config()\n",
    "print('Workspace configuration succeeded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Workspace Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice in the first line of the cell below, we can re-load the config we saved previously and then display a summary of the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "\n",
    "# Display a summary of the current environment \n",
    "output = {}\n",
    "output['SDK version'] = azureml.core.VERSION\n",
    "output['Subscription ID'] = ws.subscription_id\n",
    "output['Workspace'] = ws.name\n",
    "output['Resource Group'] = ws.resource_group\n",
    "output['Location'] = ws.location\n",
    "output['Project Directory'] = project_folder\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "pd.DataFrame(data=output, index=['']).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create a new Experiment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = Experiment(ws, experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get and explore the Vehicle Telemetry Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to download and examine the vehicle telemetry data. The model you will build will try to predict how many days until the battery has a freeze event. Which features (columns) do you think will be useful?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(data_url)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Locally train multiple models using Auto ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cells, you will use Auto ML to train multiple models, evaluate the performance and allow you to retrieve the best model that was trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the data loading script for local compute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Azure Machine Learning needs to know how to get the data to train against. You can package this logic in a script that will be executed by the AML when it starts executing the training.\n",
    "\n",
    "Run the following cells to locally create the **get_data.py** script that will be used to train the model. \n",
    "\n",
    "Observe that the get_data method returns the features (`X`) and the labels (`Y`) in an object. The method also returns the validation set, (`X_valid`) and (`y_valid`), to use for model evaluation. This structure is expected later when you will configure Auto ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create project folder\n",
    "if not os.path.exists(project_folder):\n",
    "    os.makedirs(project_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $project_folder/get_data.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def get_data():\n",
    "    \n",
    "    data_url = \"https://databricksdemostore.blob.core.windows.net/data/connected-car/training-formatted.csv\"\n",
    "    data = pd.read_csv(data_url)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(data.iloc[:,1:73], \n",
    "                                                        data.iloc[:,0].values.flatten(),\n",
    "                                                        random_state=0)\n",
    "\n",
    "    return { \"X\": X_train, \"y\": y_train, \"X_valid\":  X_test, \"y_valid\": y_test}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate an Automated ML Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to configure the Auto ML run. In short what you are configuring here is the training of a regressor model that will attempt to predict the value of the first feature (`Survival_in_days`) based on all the other features in the data set. The run is configured to try at most 3 iterations where no iteration can run longer that 2 minutes. \n",
    "\n",
    "Additionally, the data will be automatically pre-processed in different ways as a part of the automated model training (as indicated by the `preprocess` attribute having a value of `True`). This is a very powerful feature of Auto ML as it tries many best practices approaches for you, and saves you a lot of time and effort in the process. When you look at important features from the model explainer you will observe the preprocessed features the Auto ML created for the best run.\n",
    "\n",
    "The goal of Auto ML in this case is to find the best models that result, as measure by the normalized root mean squared error metric (as indicated by the `primary_metric` attribute). The error is basically a measure of what the model predicts versus what was provided as the \"answer\" in the training data. In short, AutoML will try to get the error as low as possible when trying its combination of approaches.  \n",
    "\n",
    "The local path to the script you created to retrieve the data is supplied to the AutoMLConfig, ensuring the file is made available during training.\n",
    "\n",
    "In general, the AutoMLConfig is very flexible, allowing you to specify all of the following:\n",
    "- Task type (classification, regression, forecasting)\n",
    "- Number of algorithm iterations and maximum time per iteration\n",
    "- Accuracy metric to optimize\n",
    "- Algorithms to blacklist (skip)/whitelist (include)\n",
    "- Number of cross-validations\n",
    "- Compute targets\n",
    "- Training data\n",
    "\n",
    "Run the following cell to create the configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl_config = AutoMLConfig(task = 'regression',\n",
    "                             iterations = 3,\n",
    "                             iteration_timeout_minutes = 2, \n",
    "                             max_cores_per_iteration = 10,\n",
    "                             preprocess= True,\n",
    "                             primary_metric='normalized_root_mean_squared_error',\n",
    "                             model_explainability=True,\n",
    "                             debug_log = 'automl.log',\n",
    "                             verbosity = logging.DEBUG,\n",
    "                             data_script = project_folder + \"/get_data.py\",\n",
    "                             path = project_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run our Auto ML Experiment on Local Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_run = experiment.submit(automl_config, show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List the Experiments from your Workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Azure Machine Learning SDK, you can retrieve any of the experiments in your Workspace and drill into the details of any runs the experiment contains. Run the following cell to explore the number of runs by experiment name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "experiment_list = Experiment.list(workspace=ws)\n",
    "\n",
    "summary_df = pd.DataFrame(index = ['No of Runs'])\n",
    "pattern = re.compile('^AutoML_[^_]*$')\n",
    "for experiment in experiment_list:\n",
    "    all_runs = list(experiment.get_runs())\n",
    "    automl_runs = []\n",
    "    for run in all_runs:\n",
    "        if(pattern.match(run.id)):\n",
    "            automl_runs.append(run)    \n",
    "    summary_df[experiment.name] = [len(automl_runs)]\n",
    "    \n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "summary_df.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List the Automated ML Runs for the Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, you can view all of the runs that ran supporting Auto ML:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "proj = ws.experiments[experiment_name]\n",
    "summary_df = pd.DataFrame(index = ['Type', 'Status', 'Primary Metric', 'Iterations', 'Compute', 'Name'])\n",
    "pattern = re.compile('^AutoML_[^_]*$')\n",
    "all_runs = list(proj.get_runs(properties={'azureml.runsource': 'automl'}))\n",
    "for run in all_runs:\n",
    "    if(pattern.match(run.id)):\n",
    "        properties = run.get_properties()\n",
    "        tags = run.get_tags()\n",
    "        amlsettings = eval(properties['RawAMLSettingsString'])\n",
    "        if 'iterations' in tags:\n",
    "            iterations = tags['iterations']\n",
    "        else:\n",
    "            iterations = properties['num_iterations']\n",
    "        summary_df[run.id] = [amlsettings['task_type'], run.get_details()['status'], properties['primary_metric'], iterations, properties['target'], amlsettings['name']]\n",
    "    \n",
    "from IPython.display import HTML\n",
    "projname_html = HTML(\"<h3>{}</h3>\".format(proj.name))\n",
    "\n",
    "from IPython.display import display\n",
    "display(projname_html)\n",
    "display(summary_df.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Automated ML Run Details\n",
    "For a particular run, you can display the details of how the run performed against the performance metric. The Azure Machine Learning SDK includes a built-in widget that graphically summarizes the run. \n",
    "\n",
    "Execute the following cell to see it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = local_run.id\n",
    "\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "experiment = Experiment(ws, experiment_name)\n",
    "ml_run = AutoMLRun(experiment=experiment, run_id=run_id)\n",
    "\n",
    "RunDetails(ml_run).show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the best run and the trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point you have multiple runs, each with a different trained models. How can you get the model that performed the best? Run the following cells to learn how."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_run, fitted_model = local_run.get_output()\n",
    "print(best_run)\n",
    "print(fitted_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Best Model's explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the explanation from the best_run. And explanation information includes:\n",
    "\n",
    "1. shap_values: The explanation information generated by shap lib\n",
    "2. expected_values: The expected value of the model applied to set of X_train data.\n",
    "3. overall_summary: The model level feature importance values sorted in descending order\n",
    "4. overall_imp: The feature names sorted in the same order as in overall_summary\n",
    "5. per_class_summary: The class level feature importance values sorted in descending order. Only available for the classification case\n",
    "6. per_class_imp: The feature names sorted in the same order as in per_class_summary. Only available for the classification case\n",
    "\n",
    "In the case we are only interested in looking at **overall_summary** and **overall_imp** to observe the top 10 features by their importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, overall_summary, overall_imp, _, _ = retrieve_model_explanation(best_run)\n",
    "\n",
    "important_features = pd.DataFrame(data = {'feature': overall_imp[0:10], 'value': overall_summary[0:10]},\n",
    "                                 index = [i for i in range(1, 11)])\n",
    "print('Top 10 important features')\n",
    "print(important_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions on the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to download and examine new vehicle telemetry dataset that was not used in model training or model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(test_data_url)\n",
    "test_data = test_data.drop(columns=[\"Car_ID\", \"Battery_Age\"])\n",
    "test_data.rename(columns={'Twelve_hourly_temperature_forecast_for_next_31_days_reversed': \n",
    "                          'Twelve_hourly_temperature_history_for_last_31_days_before_death_last_recording_first'}, \n",
    "                 inplace=True)\n",
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the fitted model to make **battery life** predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = fitted_model.predict(test_data)\n",
    "predictions = pd.DataFrame(predictions, columns=['Battery Life Predictions'])\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Explanation on Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain the model with the new test data - dataset that was not used in model training or model evaluation. The **explain_model** call takes in two parameters\n",
    "1. A matrix of feature vector examples for initializing the explainer.\n",
    "2. A matrix of feature vector examples on which to explain the model's output.\n",
    "\n",
    "To initialize the explainer we will use the original training set data and use the test data to retrive the model explanation. \n",
    "\n",
    "In this example, let's observe the top 10 features by their importance when predicting one of the test samples. We select the row 7 (index 6) of the test data that predicted the highest battery life of 2195."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the row 7 (index 6) from the test data\n",
    "row_7 = test_data.iloc[[6]]\n",
    "\n",
    "# Initialize with the original test features data.iloc[:,1:73] and return the model explanation for row_7\n",
    "_, _, overall_summary, overall_imp, _, _ = explain_model(fitted_model, data.iloc[:,1:73], row_7)\n",
    "\n",
    "important_features_test = pd.DataFrame(data = {'feature': overall_imp[0:10], 'value': overall_summary[0:10]},\n",
    "                                 index = [i for i in range(1, 11)])\n",
    "print('Top 10 important features for row 7')\n",
    "print(important_features_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the explain_model output, we can see that the following orginal features had the greatest influence in the row 7 prediction:\n",
    "\n",
    "1. Province\n",
    "2. Battery_Rated_Cycles\n",
    "3. Trip_Length_Mean\n",
    "4. Car_Has_EcoStart\n",
    "5. Alternator_Efficiency\n",
    "\n",
    "Let's look the origial important feature values and the corresponding model predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([test_data[['Province', 'Battery_Rated_Cycles', 'Trip_Length_Mean', 'Car_Has_EcoStart', \n",
    "                      'Alternator_Efficiency']], predictions], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For row 7 / index 6, The Battery_Rated_Cycles value 300 was highest amongst the test data, whereas, the other values such as Trip_Length_Mean 14.07 and Alternator_Efficiency 0.83 were close to the respective mean values of the test set. Thus, it appears that for row 7 / index 6, the two most differentiating features were the **Province: Marseille** and **Battery_Rated_Cycles** and these features were also identified as the top two important features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (azure_automl)",
   "language": "python",
   "name": "azure_automl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
