{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Predicting NYC Taxi Fares And Model Explainer"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "In this quickstart, we will be using a subset of NYC Taxi & Limousine Commission - green taxi trip records available from [Azure Open Datasets](https://azure.microsoft.com/en-us/services/open-datasets/). The data is enriched with holiday and weather data. We will use data transformations and the GradientBoostingRegressor algorithm from the scikit-learn library to train a regression model to predict taxi fares in New York City based on input features such as, number of passengers, trip distance, datetime, holiday information and weather information.\n\nThe primary goal of this quickstart is to explain the predictions made by our trained model with the various [Azure Model Interpretability](https://docs.microsoft.com/en-us/azure/machine-learning/service/machine-learning-interpretability-explainability) packages of the Azure Machine Learning Python SDK."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Azure Machine Learning and Model Interpretability SDK-specific Imports"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "import os\nimport numpy as np\nimport pandas as pd\nimport pickle\nimport sklearn\nfrom sklearn.externals import joblib\nimport math\n\nprint(\"pandas version: {} numpy version: {}\".format(pd.__version__, np.__version__))\n\nsklearn_version = sklearn.__version__\nprint('The scikit-learn version is {}.'.format(sklearn_version))\n\nimport azureml\nfrom azureml.core import Workspace, Experiment, Run\nfrom azureml.core.model import Model\n\nfrom interpret.ext.blackbox import TabularExplainer\nfrom azureml.interpret.scoring.scoring_explainer import TreeScoringExplainer, save\n\nprint('The azureml.core version is {}.'.format(azureml.core.VERSION))",
      "execution_count": 1,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "pandas version: 0.23.4 numpy version: 1.16.2\nThe scikit-learn version is 0.20.3.\nThe azureml.core version is 1.0.74.\n"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Setup\nTo begin, you will need to provide the following information about your Azure Subscription.\n\n**If you are using your own Azure subscription, please provide names for subscription_id, resource_group, workspace_name and workspace_region to use.** Note that the workspace needs to be of type [Machine Learning Workspace](https://docs.microsoft.com/en-us/azure/machine-learning/service/setup-create-workspace).\n\n**If an environment is provided to you be sure to replace XXXXX in the values below with your unique identifier.**\n\nIn the following cell, be sure to set the values for `subscription_id`, `resource_group`, `workspace_name` and `workspace_region` as directed by the comments (*these values can be acquired from the Azure Portal*).\n\nTo get these values, do the following:\n1. Navigate to the Azure Portal and login with the credentials provided.\n2. From the left hand menu, under Favorites, select `Resource Groups`.\n3. In the list, select the resource group with the name similar to `XXXXX`.\n4. From the Overview tab, capture the desired values.\n\nExecute the following cell by selecting the `>|Run` button in the command bar above."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "#Provide the Subscription ID of your existing Azure subscription\nsubscription_id = \"\" # <- needs to be the subscription with the Quick-Starts resource group\n\n#Provide values for the existing Resource Group \nresource_group = \"Quick-Starts-XXXXX\" # <- replace XXXXX with your unique identifier\n\n#Provide the Workspace Name and Azure Region of the Azure Machine Learning Workspace\nworkspace_name = \"quick-starts-ws-XXXXX\" # <- replace XXXXX with your unique identifier\nworkspace_region = \"eastus\" # <- region of your Quick-Starts resource group \n\nexperiment_name = \"quick-starts-explain\"",
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Create and connect to an Azure Machine Learning Workspace\n\nRun the following cell to create a new Azure Machine Learning **Workspace** and save the configuration to disk (next to the Jupyter notebook). \n\n**Important Note**: You will be prompted to login in the text that is output below the cell. Be sure to navigate to the URL displayed and enter the code that is provided. Once you have entered the code, return to this notebook and wait for the output to read `Workspace configuration succeeded`."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "ws = Workspace.create(\n    name = workspace_name,\n    subscription_id = subscription_id,\n    resource_group = resource_group, \n    location = workspace_region,\n    exist_ok = True)\n\nws.write_config()\nprint('Workspace configuration succeeded')",
      "execution_count": 4,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Workspace configuration succeeded\n"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Train the Model"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn_pandas import DataFrameMapper\nfrom sklearn.metrics import mean_squared_error\n\ndata_url = ('https://quickstartsws9073123377.blob.core.windows.net/'\n            'azureml-blobstore-0d1c4218-a5f9-418b-bf55-902b65277b85/'\n            'quickstarts/nyc-taxi-data/nyc-taxi-sample-data.csv')\n\ndf = pd.read_csv(data_url)\nx_df = df.drop(['totalAmount'], axis=1)\ny_df = df['totalAmount']\n\nX_train, X_test, y_train, y_test = train_test_split(x_df, y_df, test_size=0.2, random_state=0)\n\ncategorical = ['normalizeHolidayName', 'isPaidTimeOff']\nnumerical = ['vendorID', 'passengerCount', 'tripDistance', 'hour_of_day', 'day_of_week', \n             'day_of_month', 'month_num', 'snowDepth', 'precipTime', 'precipDepth', 'temperature']\n\nnumeric_transformations = [([f], Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler())])) for f in numerical]\n    \ncategorical_transformations = [([f], OneHotEncoder(handle_unknown='ignore', sparse=False)) for f in categorical]\n\ntransformations = numeric_transformations + categorical_transformations\n\nclf = Pipeline(steps=[('preprocessor', DataFrameMapper(transformations)),\n                      ('regressor', GradientBoostingRegressor())])\n\nclf.fit(X_train, y_train)\n\ny_predict = clf.predict(X_test)\ny_actual = y_test.values.flatten().tolist()\nrmse = math.sqrt(mean_squared_error(y_actual, y_predict))\nprint('The RMSE score on test data for GradientBoostingRegressor: ', rmse)",
      "execution_count": 5,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "The RMSE score on test data for GradientBoostingRegressor:  4.3216013442932075\n"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Global Explanation Using TabularExplainer\n\n**Global Model Explanation** is a holistic understanding of how the model makes decisions. It provides you with insights on what features are most important and their relative strengths in making model predictions.\n\n[TabularExplainer](https://docs.microsoft.com/en-us/python/api/azureml-explain-model/azureml.explain.model.tabularexplainer?view=azure-ml-py) uses one of three explainers: TreeExplainer, DeepExplainer, or KernelExplainer, and is automatically selecting the most appropriate one for our use case. You can learn more about the underlying model explainers at [Azure Model Interpretability](https://docs.microsoft.com/en-us/azure/machine-learning/service/machine-learning-interpretability-explainability).\n\nTo initialize an explainer object, you need to pass your model and some training data to the explainer's constructor.\n\n*Note that you can pass in your feature transformation pipeline to the explainer to receive explanations in terms of the raw features before the transformation (rather than engineered features).*"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# \"features\" and \"classes\" fields are optional\ntabular_explainer = TabularExplainer(clf.steps[-1][1], initialization_examples=X_train, \n                                     features=X_train.columns,  transformations=transformations)",
      "execution_count": 6,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Get the global feature importance values\n\nRun the below cell and observe the sorted global feature importance. You will note that `tripDistance` is the most important feature in predicting the taxi fares, followed by `hour_of_day`, and `day_of_week`."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# You can use the training data or the test data here\nglobal_explanation = tabular_explainer.explain_global(X_test)\n# Sorted feature importance values and feature names\nsorted_global_importance_values = global_explanation.get_ranked_global_values()\nsorted_global_importance_names = global_explanation.get_ranked_global_names()\ndict(zip(sorted_global_importance_names, sorted_global_importance_values))",
      "execution_count": 7,
      "outputs": [
        {
          "data": {
            "text/plain": "{'tripDistance': 6.690343998817808,\n 'hour_of_day': 0.43986432748994014,\n 'day_of_week': 0.2465941349727387,\n 'precipDepth': 0.04291348242553137,\n 'passengerCount': 0.03621378189569159,\n 'temperature': 0.028714306953134115,\n 'day_of_month': 0.024794730200096447,\n 'snowDepth': 0.011707454686670362,\n 'isPaidTimeOff': 0.01118206593843017,\n 'month_num': 0.006406974738323228,\n 'normalizeHolidayName': 0.005372405952633575,\n 'vendorID': 0.0020368912914650547,\n 'precipTime': 0.001010658748108876}"
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Local Explanation\n\nYou can use the [TabularExplainer](https://docs.microsoft.com/en-us/python/api/azureml-explain-model/azureml.explain.model.tabularexplainer?view=azure-ml-py) for a single prediction. You can focus on a single instance and examine model prediction for this input, and explain why.\n\nWe will create two sample inputs to explain the individual predictions.\n\n- **Data 1**\n - 4 Passengers at 3:00PM, Friday July 5th, temperature 80F, travelling 10 miles\n\n- **Data 2**\n - 1 Passenger at 6:00AM, Monday January 20th, rainy, temperature 35F, travelling 5 miles"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Create the test dataset\ncolumns = ['vendorID', 'passengerCount', 'tripDistance', 'hour_of_day', 'day_of_week', 'day_of_month', \n           'month_num', 'normalizeHolidayName', 'isPaidTimeOff', 'snowDepth', 'precipTime', \n           'precipDepth', 'temperature']\n\ndata = [[1, 4, 10, 15, 4, 5, 7, 'None', False, 0, 0.0, 0.0, 80], \n        [1, 1, 5, 6, 0, 20, 1, 'Martin Luther King, Jr. Day', True, 0, 2.0, 3.0, 35]]\n\ndata_df = pd.DataFrame(data, columns = columns)",
      "execution_count": 8,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# explain the test data\nlocal_explanation = tabular_explainer.explain_local(data_df)\n\n# sorted feature importance values and feature names\nsorted_local_importance_names = local_explanation.get_ranked_local_names()\nsorted_local_importance_values = local_explanation.get_ranked_local_values()\n\nresults = pd.DataFrame([sorted_local_importance_names[0][0:5], sorted_local_importance_values[0][0:5], \n                        sorted_local_importance_names[1][0:5], sorted_local_importance_values[1][0:5]], \n                       columns = ['1st', '2nd', '3rd', '4th', '5th'], \n                       index = ['Data 1', '', 'Data 2', ''])\nprint('Top 5 Local Feature Importance')\nresults",
      "execution_count": 9,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Top 5 Local Feature Importance\n"
        },
        {
          "data": {
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>1st</th>\n      <th>2nd</th>\n      <th>3rd</th>\n      <th>4th</th>\n      <th>5th</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Data 1</th>\n      <td>tripDistance</td>\n      <td>hour_of_day</td>\n      <td>passengerCount</td>\n      <td>day_of_week</td>\n      <td>temperature</td>\n    </tr>\n    <tr>\n      <th></th>\n      <td>23.7412</td>\n      <td>0.814436</td>\n      <td>0.404508</td>\n      <td>0.131821</td>\n      <td>0.125112</td>\n    </tr>\n    <tr>\n      <th>Data 2</th>\n      <td>tripDistance</td>\n      <td>temperature</td>\n      <td>day_of_week</td>\n      <td>month_num</td>\n      <td>precipTime</td>\n    </tr>\n    <tr>\n      <th></th>\n      <td>7.7225</td>\n      <td>0.088575</td>\n      <td>0.0778875</td>\n      <td>0.0185127</td>\n      <td>7.48644e-05</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "                 1st          2nd             3rd          4th          5th\nData 1  tripDistance  hour_of_day  passengerCount  day_of_week  temperature\n             23.7412     0.814436        0.404508     0.131821     0.125112\nData 2  tripDistance  temperature     day_of_week    month_num   precipTime\n              7.7225     0.088575       0.0778875    0.0185127  7.48644e-05"
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "As we saw from the Global Explanation that the **tripDistance** is the most important global feature. Other than `tripDistance`, the rest of the top 5 important features were different for the two samples.\n\n- Data 1: Passenger count 4 and 3:00 PM on Friday were also important features in the prediction.\n- Data 2: The weather-related features (rainy, temperature 35F), day of the week (Monday) and month (January) were also important."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Interpretability in Inference\n\nIn the next part, we will deploy the explainer along with the model to be used at scoring time to make predictions and provide local explanation."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Create a Scoring Explainer\n\nUse the **TabularExplainer** from the **interpret.ext.blackbox** package to create the **Tree Scoring Explainer** that can be deployed with the trained model."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "scoring_explainer = TreeScoringExplainer(tabular_explainer)",
      "execution_count": 10,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Register the Trained Model and the Scoring Explainer with Azure Machine Learning Service\n\nRun the next set of cells to register the two models with Azure Machine Learning Service."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Start an Experiment Run"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "experiment = Experiment(ws, experiment_name)\nrun = experiment.start_logging()",
      "execution_count": 11,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Save the two models to local disk"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "os.makedirs('./outputs', exist_ok=True)\n\n# save the model\nmodel_name = 'nyc-taxi-fare'\nmodel_file_name = model_name + '.pkl'\noutput_model_file_path = os.path.join('./outputs', model_file_name)\njoblib.dump(clf, open(output_model_file_path,'wb'))\n\n# save the scoring explainer - the file name will be scoring_explainer.pkl\nscoring_explainer_name = 'nyc-taxi-fare-explainer'\nscoring_explainer_file_name = 'scoring_explainer.pkl'\nscoring_explainer_file_path = os.path.join('./outputs', scoring_explainer_file_name)\nsave(scoring_explainer, directory='./outputs', exist_ok=True)",
      "execution_count": 12,
      "outputs": [
        {
          "data": {
            "text/plain": "'./outputs/scoring_explainer.pkl'"
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Register the Models"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Upload and register this version of the model with Azure Machine Learning service\nmodel_destination_path = os.path.join('outputs', model_name)\nrun.upload_file(model_destination_path, output_model_file_path) # destination, source\nregistered_model = run.register_model(model_name=model_name, model_path=model_destination_path)\n\nscoring_explainer_destination_path = os.path.join('outputs', scoring_explainer_file_name)\nrun.upload_file(scoring_explainer_destination_path, scoring_explainer_file_path) # destination, source\nregistered_scoring_explainer = run.register_model(model_name=scoring_explainer_name, \n                                                  model_path=scoring_explainer_destination_path)\n\nprint(\"Model registered: {} \\nModel Version: {}\".format(registered_model.name, registered_model.version))\nprint(\"Explainer Model registered: {} \\nExplainer Model Version: {}\".format(registered_scoring_explainer.name, \n                                                                            registered_scoring_explainer.version))",
      "execution_count": 13,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Model registered: nyc-taxi-fare \nModel Version: 4\nExplainer Model registered: nyc-taxi-fare-explainer \nExplainer Model Version: 3\n"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Complete the Experiment Run"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "run.complete()",
      "execution_count": 14,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Create the Scoring Script"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "%%writefile score.py\nimport json\nimport numpy as np\nimport pandas as pd\nimport sklearn\nfrom sklearn.externals import joblib\nfrom azureml.core.model import Model\n\ncolumns = ['vendorID', 'passengerCount', 'tripDistance', 'hour_of_day', 'day_of_week', 'day_of_month', \n           'month_num', 'normalizeHolidayName', 'isPaidTimeOff', 'snowDepth', 'precipTime', \n           'precipDepth', 'temperature']\n\ndef init():\n\n    global original_model\n    global scoring_explainer\n    \n    print('The scikit-learn version is {}.'.format(sklearn.__version__))\n\n    # Retrieve the path to the model file using the model name\n    # Assume original model is named original_prediction_model\n    original_model_path = Model.get_model_path('nyc-taxi-fare')\n    scoring_explainer_path = Model.get_model_path('nyc-taxi-fare-explainer')\n\n    original_model = joblib.load(original_model_path)\n    print('model loaded')\n    scoring_explainer = joblib.load(scoring_explainer_path)\n    print('explainer loaded')\n\ndef run(input_json):\n    # Get predictions and explanations for each data point\n    inputs = json.loads(input_json)\n    data_df = pd.DataFrame(np.array(inputs).reshape(-1, len(columns)), columns = columns)\n    # Make prediction\n    predictions = original_model.predict(data_df)\n    # Retrieve model explanations\n    local_importance_values = scoring_explainer.explain(data_df)\n    # You can return any data type as long as it is JSON-serializable\n    return {'predictions': predictions.tolist(), 'local_importance_values': local_importance_values}",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Writing score.py\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Package Model\n\nRun the next two cells to create the deployment **Image**. This can take up to 5 minutes.\n\n*WARNING: to install, g++ needs to be available on the Docker image and is not by default. Thus, we will create a custom dockerfile with g++ installed.*"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "%%writefile dockerfile\nRUN apt-get update && apt-get install -y g++",
      "execution_count": 16,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Overwriting dockerfile\n"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# create a Conda dependencies environment file\nprint(\"Creating conda dependencies file locally...\")\nfrom azureml.core.conda_dependencies import CondaDependencies\nconda_packages = ['numpy==1.16.2', 'pandas==0.23.0', 'scikit-learn==0.20.3']\npip_packages = ['sklearn_pandas==1.7.0', 'azureml-defaults', 'azureml-core', 'azureml-interpret']\n\nmycondaenv = CondaDependencies.create(conda_packages=conda_packages, pip_packages=pip_packages)\n\nconda_file = 'dependencies.yml'\nwith open(conda_file, 'w') as f:\n    f.write(mycondaenv.serialize_to_string())\n\nruntime = 'python'\n\n# create container image configuration\nprint(\"Creating container image configuration...\")\nfrom azureml.core.image import ContainerImage\nimage_config = ContainerImage.image_configuration(execution_script = 'score.py', \n                                                  docker_file = 'dockerfile', \n                                                  runtime = runtime, \n                                                  conda_file = conda_file)\n\n# create the image\nimage_name = 'nyc-taxi-fare-image'\n\nfrom azureml.core import Image\nimage = Image.create(name=image_name, models=[registered_model, registered_scoring_explainer], \n                     image_config=image_config, workspace=ws)\n\n# wait for image creation to finish\nimage.wait_for_creation(show_output=True)",
      "execution_count": 17,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Creating conda dependencies file locally...\nCreating container image configuration...\nCreating image\nRunning..................................................................\nSucceeded\nImage creation operation finished for image nyc-taxi-fare-image:3, operation \"Succeeded\"\n"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Deploy Model to Azure Container Instance (ACI) as a Web Service"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "from azureml.core.webservice import AciWebservice, Webservice\n\naci_name = 'nyc-taxi-aci-cluster01'\n\naci_config = AciWebservice.deploy_configuration(\n    cpu_cores = 1, \n    memory_gb = 1, \n    tags = {'name': aci_name}, \n    description = 'NYC Taxi Fare Predictor Web Service')\n\nservice_name = 'nyc-taxi-explainer-service'\n\naci_service = Webservice.deploy_from_image(deployment_config=aci_config, \n                                           image=image, \n                                           name=service_name, \n                                           workspace=ws)\n\naci_service.wait_for_deployment(show_output=True)",
      "execution_count": 18,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Running...................\nSucceeded\nACI service creation operation finished, operation \"Succeeded\"\n"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Test Deployment\n\nObserve that the **Scoring Service** return both prediction and local importance values."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "import json\n\ndata1 = [1, 2, 5, 9, 4, 27, 5, 'Memorial Day', True, 0, 0.0, 0.0, 65]\n\ndata2 = [[1, 3, 10, 15, 4, 27, 7, 'None', False, 0, 2.0, 1.0, 80], \n         [1, 2, 5, 9, 4, 27, 5, 'Memorial Day', True, 0, 0.0, 0.0, 65]]\n\nresult = aci_service.run(json.dumps(data1))\nprint('Predictions for data1')\nprint(result)\n\nresult = aci_service.run(json.dumps(data2))\nprint('Predictions for data2')\nprint(result)",
      "execution_count": 19,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Predictions for data1\n{'predictions': [23.46649044047035], 'local_importance_values': [[-0.002514044259156158, 0.08221763468127499, 8.256150755576948, 0.2365744085521227, 0.3037989046157431, -0.0002635950579767754, -0.0019577864175662728, -0.17295167740578782, -0.08906120188890428, -0.005750894587797881, -0.0002414232770696094, -0.017012212419371377, 0.10125883307554298]]}\nPredictions for data2\n{'predictions': [40.29083650824729, 23.46649044047035], 'local_importance_values': [[-0.004473427200976978, 0.4050438135402611, 24.007431201903405, 0.8002676204500312, 0.14574696635630488, 0.10580253739165334, -0.0019577864175662728, 0.003786777701442244, -0.08808626740739736, 0.003348032366657192, -0.0002414232770696094, 0.012426040616013611, 0.12549973935974093], [-0.002514044259156158, 0.08221763468127499, 8.256150755576948, 0.2365744085521227, 0.3037989046157431, -0.0002635950579767754, -0.0019577864175662728, -0.17295167740578782, -0.08906120188890428, -0.005750894587797881, -0.0002414232770696094, -0.017012212419371377, 0.10125883307554298]]}\n"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}