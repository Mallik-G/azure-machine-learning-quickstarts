{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting NYC Taxi Fares And Model Explainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this quickstart, we will be using a subset of NYC Taxi & Limousine Commission - green taxi trip records available from [Azure Open Datasets](https://azure.microsoft.com/en-us/services/open-datasets/). The data is enriched with holiday and weather data. We will use data transformations and the GradientBoostingRegressor algorithm from the scikit-learn library to train a regression model to predict taxi fares in New York City based on input features such as, number of passengers, trip distance, datetime, holiday information and weather information.\n",
    "\n",
    "The primary goal of this quickstart is to explain the predictions made by our trained model with the various [Azure Model Interpretability](https://docs.microsoft.com/en-us/azure/machine-learning/service/machine-learning-interpretability-explainability) packages of the Azure Machine Learning Python SDK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Azure Machine Learning and Model Interpretability SDK-specific Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas version: 0.23.4 numpy version: 1.16.2\n",
      "The scikit-learn version is 0.20.3.\n",
      "The azureml.core version is 1.0.74.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import sklearn\n",
    "from sklearn.externals import joblib\n",
    "import math\n",
    "\n",
    "print(\"pandas version: {} numpy version: {}\".format(pd.__version__, np.__version__))\n",
    "\n",
    "sklearn_version = sklearn.__version__\n",
    "print('The scikit-learn version is {}.'.format(sklearn_version))\n",
    "\n",
    "import azureml\n",
    "from azureml.core import Workspace, Experiment, Run\n",
    "from azureml.core.model import Model\n",
    "\n",
    "from interpret.ext.blackbox import TabularExplainer\n",
    "from azureml.interpret.scoring.scoring_explainer import TreeScoringExplainer, save\n",
    "\n",
    "print('The azureml.core version is {}.'.format(azureml.core.VERSION))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "To begin, you will need to provide the following information about your Azure Subscription.\n",
    "\n",
    "**If you are using your own Azure subscription, please provide names for subscription_id, resource_group, workspace_name and workspace_region to use.** Note that the workspace needs to be of type [Machine Learning Workspace](https://docs.microsoft.com/en-us/azure/machine-learning/service/setup-create-workspace).\n",
    "\n",
    "**If an environment is provided to you be sure to replace XXXXX in the values below with your unique identifier.**\n",
    "\n",
    "In the following cell, be sure to set the values for `subscription_id`, `resource_group`, `workspace_name` and `workspace_region` as directed by the comments (*these values can be acquired from the Azure Portal*).\n",
    "\n",
    "To get these values, do the following:\n",
    "1. Navigate to the Azure Portal and login with the credentials provided.\n",
    "2. From the left hand menu, under Favorites, select `Resource Groups`.\n",
    "3. In the list, select the resource group with the name similar to `XXXXX`.\n",
    "4. From the Overview tab, capture the desired values.\n",
    "\n",
    "Execute the following cell by selecting the `>|Run` button in the command bar above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Provide the Subscription ID of your existing Azure subscription\n",
    "subscription_id = \"\" # <- needs to be the subscription with the Quick-Starts resource group\n",
    "\n",
    "#Provide values for the existing Resource Group \n",
    "resource_group = \"Quick-Starts-XXXXX\" # <- replace XXXXX with your unique identifier\n",
    "\n",
    "#Provide the Workspace Name and Azure Region of the Azure Machine Learning Workspace\n",
    "workspace_name = \"quick-starts-ws-XXXXX\" # <- replace XXXXX with your unique identifier\n",
    "workspace_region = \"eastus\" # <- region of your Quick-Starts resource group \n",
    "\n",
    "experiment_name = \"quick-starts-explain\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and connect to an Azure Machine Learning Workspace\n",
    "\n",
    "Run the following cell to create a new Azure Machine Learning **Workspace** and save the configuration to disk (next to the Jupyter notebook). \n",
    "\n",
    "**Important Note**: You will be prompted to login in the text that is output below the cell. Be sure to navigate to the URL displayed and enter the code that is provided. Once you have entered the code, return to this notebook and wait for the output to read `Workspace configuration succeeded`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workspace configuration succeeded\n"
     ]
    }
   ],
   "source": [
    "ws = Workspace.create(\n",
    "    name = workspace_name,\n",
    "    subscription_id = subscription_id,\n",
    "    resource_group = resource_group, \n",
    "    location = workspace_region,\n",
    "    exist_ok = True)\n",
    "\n",
    "ws.write_config()\n",
    "print('Workspace configuration succeeded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RMSE score on test data for GradientBoostingRegressor:  4.3216013442932075\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "data_url = ('https://quickstartsws9073123377.blob.core.windows.net/'\n",
    "            'azureml-blobstore-0d1c4218-a5f9-418b-bf55-902b65277b85/'\n",
    "            'quickstarts/nyc-taxi-data/nyc-taxi-sample-data.csv')\n",
    "\n",
    "df = pd.read_csv(data_url)\n",
    "x_df = df.drop(['totalAmount'], axis=1)\n",
    "y_df = df['totalAmount']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_df, y_df, test_size=0.2, random_state=0)\n",
    "\n",
    "categorical = ['normalizeHolidayName', 'isPaidTimeOff']\n",
    "numerical = ['vendorID', 'passengerCount', 'tripDistance', 'hour_of_day', 'day_of_week', \n",
    "             'day_of_month', 'month_num', 'snowDepth', 'precipTime', 'precipDepth', 'temperature']\n",
    "\n",
    "numeric_transformations = [([f], Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])) for f in numerical]\n",
    "    \n",
    "categorical_transformations = [([f], OneHotEncoder(handle_unknown='ignore', sparse=False)) for f in categorical]\n",
    "\n",
    "transformations = numeric_transformations + categorical_transformations\n",
    "\n",
    "clf = Pipeline(steps=[('preprocessor', DataFrameMapper(transformations)),\n",
    "                      ('regressor', GradientBoostingRegressor())])\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_predict = clf.predict(X_test)\n",
    "y_actual = y_test.values.flatten().tolist()\n",
    "rmse = math.sqrt(mean_squared_error(y_actual, y_predict))\n",
    "print('The RMSE score on test data for GradientBoostingRegressor: ', rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Explanation Using TabularExplainer\n",
    "\n",
    "**Global Model Explanation** is a holistic understanding of how the model makes decisions. It provides you with insights on what features are most important and their relative strengths in making model predictions.\n",
    "\n",
    "[TabularExplainer](https://docs.microsoft.com/en-us/python/api/azureml-explain-model/azureml.explain.model.tabularexplainer?view=azure-ml-py) uses one of three explainers: TreeExplainer, DeepExplainer, or KernelExplainer, and is automatically selecting the most appropriate one for our use case. You can learn more about the underlying model explainers at [Azure Model Interpretability](https://docs.microsoft.com/en-us/azure/machine-learning/service/machine-learning-interpretability-explainability).\n",
    "\n",
    "To initialize an explainer object, you need to pass your model and some training data to the explainer's constructor.\n",
    "\n",
    "*Note that you can pass in your feature transformation pipeline to the explainer to receive explanations in terms of the raw features before the transformation (rather than engineered features).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"features\" and \"classes\" fields are optional\n",
    "tabular_explainer = TabularExplainer(clf.steps[-1][1], initialization_examples=X_train, \n",
    "                                     features=X_train.columns,  transformations=transformations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the global feature importance values\n",
    "\n",
    "Run the below cell and observe the sorted global feature importance. You will note that `tripDistance` is the most important feature in predicting the taxi fares, followed by `hour_of_day`, and `day_of_week`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tripDistance': 6.690343998817808,\n",
       " 'hour_of_day': 0.43986432748994014,\n",
       " 'day_of_week': 0.2465941349727387,\n",
       " 'precipDepth': 0.04291348242553137,\n",
       " 'passengerCount': 0.03621378189569159,\n",
       " 'temperature': 0.028714306953134115,\n",
       " 'day_of_month': 0.024794730200096447,\n",
       " 'snowDepth': 0.011707454686670362,\n",
       " 'isPaidTimeOff': 0.01118206593843017,\n",
       " 'month_num': 0.006406974738323228,\n",
       " 'normalizeHolidayName': 0.005372405952633575,\n",
       " 'vendorID': 0.0020368912914650547,\n",
       " 'precipTime': 0.001010658748108876}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can use the training data or the test data here\n",
    "global_explanation = tabular_explainer.explain_global(X_test)\n",
    "# Sorted feature importance values and feature names\n",
    "sorted_global_importance_values = global_explanation.get_ranked_global_values()\n",
    "sorted_global_importance_names = global_explanation.get_ranked_global_names()\n",
    "dict(zip(sorted_global_importance_names, sorted_global_importance_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Explanation\n",
    "\n",
    "You can use the [TabularExplainer](https://docs.microsoft.com/en-us/python/api/azureml-explain-model/azureml.explain.model.tabularexplainer?view=azure-ml-py) for a single prediction. You can focus on a single instance and examine model prediction for this input, and explain why.\n",
    "\n",
    "We will create two sample inputs to explain the individual predictions.\n",
    "\n",
    "- **Data 1**\n",
    " - 4 Passengers at 3:00PM, Friday July 5th, temperature 80F, travelling 10 miles\n",
    "\n",
    "- **Data 2**\n",
    " - 1 Passenger at 6:00AM, Monday January 20th, rainy, temperature 35F, travelling 5 miles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the test dataset\n",
    "columns = ['vendorID', 'passengerCount', 'tripDistance', 'hour_of_day', 'day_of_week', 'day_of_month', \n",
    "           'month_num', 'normalizeHolidayName', 'isPaidTimeOff', 'snowDepth', 'precipTime', \n",
    "           'precipDepth', 'temperature']\n",
    "\n",
    "data = [[1, 4, 10, 15, 4, 5, 7, 'None', False, 0, 0.0, 0.0, 80], \n",
    "        [1, 1, 5, 6, 0, 20, 1, 'Martin Luther King, Jr. Day', True, 0, 2.0, 3.0, 35]]\n",
    "\n",
    "data_df = pd.DataFrame(data, columns = columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Local Feature Importance\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1st</th>\n",
       "      <th>2nd</th>\n",
       "      <th>3rd</th>\n",
       "      <th>4th</th>\n",
       "      <th>5th</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Data 1</th>\n",
       "      <td>tripDistance</td>\n",
       "      <td>hour_of_day</td>\n",
       "      <td>passengerCount</td>\n",
       "      <td>day_of_week</td>\n",
       "      <td>temperature</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>23.7412</td>\n",
       "      <td>0.814436</td>\n",
       "      <td>0.404508</td>\n",
       "      <td>0.131821</td>\n",
       "      <td>0.125112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Data 2</th>\n",
       "      <td>tripDistance</td>\n",
       "      <td>temperature</td>\n",
       "      <td>day_of_week</td>\n",
       "      <td>month_num</td>\n",
       "      <td>precipTime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>7.7225</td>\n",
       "      <td>0.088575</td>\n",
       "      <td>0.0778875</td>\n",
       "      <td>0.0185127</td>\n",
       "      <td>7.48644e-05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 1st          2nd             3rd          4th          5th\n",
       "Data 1  tripDistance  hour_of_day  passengerCount  day_of_week  temperature\n",
       "             23.7412     0.814436        0.404508     0.131821     0.125112\n",
       "Data 2  tripDistance  temperature     day_of_week    month_num   precipTime\n",
       "              7.7225     0.088575       0.0778875    0.0185127  7.48644e-05"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# explain the test data\n",
    "local_explanation = tabular_explainer.explain_local(data_df)\n",
    "\n",
    "# sorted feature importance values and feature names\n",
    "sorted_local_importance_names = local_explanation.get_ranked_local_names()\n",
    "sorted_local_importance_values = local_explanation.get_ranked_local_values()\n",
    "\n",
    "results = pd.DataFrame([sorted_local_importance_names[0][0:5], sorted_local_importance_values[0][0:5], \n",
    "                        sorted_local_importance_names[1][0:5], sorted_local_importance_values[1][0:5]], \n",
    "                       columns = ['1st', '2nd', '3rd', '4th', '5th'], \n",
    "                       index = ['Data 1', '', 'Data 2', ''])\n",
    "print('Top 5 Local Feature Importance')\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw from the Global Explanation that the **tripDistance** is the most important global feature. Other than `tripDistance`, the rest of the top 5 important features were different for the two samples.\n",
    "\n",
    "- Data 1: Passenger count 4 and 3:00 PM on Friday were also important features in the prediction.\n",
    "- Data 2: The weather-related features (rainy, temperature 35F), day of the week (Monday) and month (January) were also important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretability in Inference\n",
    "\n",
    "In the next part, we will deploy the explainer along with the model to be used at scoring time to make predictions and provide local explanation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a Scoring Explainer\n",
    "\n",
    "Use the **TabularExplainer** from the **interpret.ext.blackbox** package to create the **Tree Scoring Explainer** that can be deployed with the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_explainer = TreeScoringExplainer(tabular_explainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register the Trained Model and the Scoring Explainer with Azure Machine Learning Service\n",
    "\n",
    "Run the next set of cells to register the two models with Azure Machine Learning Service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start an Experiment Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = Experiment(ws, experiment_name)\n",
    "run = experiment.start_logging()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the two models to local disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./outputs/scoring_explainer.pkl'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.makedirs('./outputs', exist_ok=True)\n",
    "\n",
    "# save the model\n",
    "model_name = 'nyc-taxi-fare'\n",
    "model_file_name = model_name + '.pkl'\n",
    "output_model_file_path = os.path.join('./outputs', model_file_name)\n",
    "joblib.dump(clf, open(output_model_file_path,'wb'))\n",
    "\n",
    "# save the scoring explainer - the file name will be scoring_explainer.pkl\n",
    "scoring_explainer_name = 'nyc-taxi-fare-explainer'\n",
    "scoring_explainer_file_name = 'scoring_explainer.pkl'\n",
    "scoring_explainer_file_path = os.path.join('./outputs', scoring_explainer_file_name)\n",
    "save(scoring_explainer, directory='./outputs', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Register the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model registered: nyc-taxi-fare \n",
      "Model Version: 4\n",
      "Explainer Model registered: nyc-taxi-fare-explainer \n",
      "Explainer Model Version: 3\n"
     ]
    }
   ],
   "source": [
    "# Upload and register this version of the model with Azure Machine Learning service\n",
    "model_destination_path = os.path.join('outputs', model_name)\n",
    "run.upload_file(model_destination_path, output_model_file_path) # destination, source\n",
    "registered_model = run.register_model(model_name=model_name, model_path=model_destination_path)\n",
    "\n",
    "scoring_explainer_destination_path = os.path.join('outputs', scoring_explainer_file_name)\n",
    "run.upload_file(scoring_explainer_destination_path, scoring_explainer_file_path) # destination, source\n",
    "registered_scoring_explainer = run.register_model(model_name=scoring_explainer_name, \n",
    "                                                  model_path=scoring_explainer_destination_path)\n",
    "\n",
    "print(\"Model registered: {} \\nModel Version: {}\".format(registered_model.name, registered_model.version))\n",
    "print(\"Explainer Model registered: {} \\nExplainer Model Version: {}\".format(registered_scoring_explainer.name, \n",
    "                                                                            registered_scoring_explainer.version))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Complete the Experiment Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.complete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Scoring Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting score.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile score.py\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.externals import joblib\n",
    "from azureml.core.model import Model\n",
    "\n",
    "columns = ['vendorID', 'passengerCount', 'tripDistance', 'hour_of_day', 'day_of_week', 'day_of_month', \n",
    "           'month_num', 'normalizeHolidayName', 'isPaidTimeOff', 'snowDepth', 'precipTime', \n",
    "           'precipDepth', 'temperature']\n",
    "\n",
    "def init():\n",
    "\n",
    "    global original_model\n",
    "    global scoring_explainer\n",
    "    \n",
    "    print('The scikit-learn version is {}.'.format(sklearn.__version__))\n",
    "\n",
    "    # Retrieve the path to the model file using the model name\n",
    "    # Assume original model is named original_prediction_model\n",
    "    original_model_path = Model.get_model_path('nyc-taxi-fare')\n",
    "    scoring_explainer_path = Model.get_model_path('nyc-taxi-fare-explainer')\n",
    "\n",
    "    original_model = joblib.load(original_model_path)\n",
    "    print('model loaded')\n",
    "    scoring_explainer = joblib.load(scoring_explainer_path)\n",
    "    print('explainer loaded')\n",
    "\n",
    "def run(input_json):\n",
    "    # Get predictions and explanations for each data point\n",
    "    inputs = json.loads(input_json)\n",
    "    data_df = pd.DataFrame(np.array(inputs).reshape(-1, len(columns)), columns = columns)\n",
    "    # Make prediction\n",
    "    predictions = original_model.predict(data_df)\n",
    "    # Retrieve model explanations\n",
    "    local_importance_values = scoring_explainer.explain(data_df)\n",
    "    # You can return any data type as long as it is JSON-serializable\n",
    "    return {'predictions': predictions.tolist(), 'local_importance_values': local_importance_values}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Package Model\n",
    "\n",
    "Run the next two cells to create the deployment **Image**. This can take up to 5 minutes.\n",
    "\n",
    "*WARNING: to install, g++ needs to be available on the Docker image and is not by default. Thus, we will create a custom dockerfile with g++ installed.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile dockerfile\n",
    "RUN apt-get update && apt-get install -y g++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating conda dependencies file locally...\n",
      "Creating container image configuration...\n",
      "Creating image\n",
      "Running..................................................................\n",
      "Succeeded\n",
      "Image creation operation finished for image nyc-taxi-fare-image:3, operation \"Succeeded\"\n"
     ]
    }
   ],
   "source": [
    "# create a Conda dependencies environment file\n",
    "print(\"Creating conda dependencies file locally...\")\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "conda_packages = ['numpy==1.16.2', 'pandas==0.23.0', 'scikit-learn==0.20.3']\n",
    "pip_packages = ['sklearn_pandas==1.7.0', 'azureml-defaults', 'azureml-core', 'azureml-interpret']\n",
    "\n",
    "mycondaenv = CondaDependencies.create(conda_packages=conda_packages, pip_packages=pip_packages)\n",
    "\n",
    "conda_file = 'dependencies.yml'\n",
    "with open(conda_file, 'w') as f:\n",
    "    f.write(mycondaenv.serialize_to_string())\n",
    "\n",
    "runtime = 'python'\n",
    "\n",
    "# create container image configuration\n",
    "print(\"Creating container image configuration...\")\n",
    "from azureml.core.image import ContainerImage\n",
    "image_config = ContainerImage.image_configuration(execution_script = 'score.py', \n",
    "                                                  docker_file = 'dockerfile', \n",
    "                                                  runtime = runtime, \n",
    "                                                  conda_file = conda_file)\n",
    "\n",
    "# create the image\n",
    "image_name = 'nyc-taxi-fare-image'\n",
    "\n",
    "from azureml.core import Image\n",
    "image = Image.create(name=image_name, models=[registered_model, registered_scoring_explainer], \n",
    "                     image_config=image_config, workspace=ws)\n",
    "\n",
    "# wait for image creation to finish\n",
    "image.wait_for_creation(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy Model to Azure Container Instance (ACI) as a Web Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running...................\n",
      "Succeeded\n",
      "ACI service creation operation finished, operation \"Succeeded\"\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.webservice import AciWebservice, Webservice\n",
    "\n",
    "aci_name = 'nyc-taxi-aci-cluster01'\n",
    "\n",
    "aci_config = AciWebservice.deploy_configuration(\n",
    "    cpu_cores = 1, \n",
    "    memory_gb = 1, \n",
    "    tags = {'name': aci_name}, \n",
    "    description = 'NYC Taxi Fare Predictor Web Service')\n",
    "\n",
    "service_name = 'nyc-taxi-explainer-service'\n",
    "\n",
    "aci_service = Webservice.deploy_from_image(deployment_config=aci_config, \n",
    "                                           image=image, \n",
    "                                           name=service_name, \n",
    "                                           workspace=ws)\n",
    "\n",
    "aci_service.wait_for_deployment(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Deployment\n",
    "\n",
    "Observe that the **Scoring Service** return both prediction and local importance values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for data1\n",
      "{'predictions': [23.46649044047035], 'local_importance_values': [[-0.002514044259156158, 0.08221763468127499, 8.256150755576948, 0.2365744085521227, 0.3037989046157431, -0.0002635950579767754, -0.0019577864175662728, -0.17295167740578782, -0.08906120188890428, -0.005750894587797881, -0.0002414232770696094, -0.017012212419371377, 0.10125883307554298]]}\n",
      "Predictions for data2\n",
      "{'predictions': [40.29083650824729, 23.46649044047035], 'local_importance_values': [[-0.004473427200976978, 0.4050438135402611, 24.007431201903405, 0.8002676204500312, 0.14574696635630488, 0.10580253739165334, -0.0019577864175662728, 0.003786777701442244, -0.08808626740739736, 0.003348032366657192, -0.0002414232770696094, 0.012426040616013611, 0.12549973935974093], [-0.002514044259156158, 0.08221763468127499, 8.256150755576948, 0.2365744085521227, 0.3037989046157431, -0.0002635950579767754, -0.0019577864175662728, -0.17295167740578782, -0.08906120188890428, -0.005750894587797881, -0.0002414232770696094, -0.017012212419371377, 0.10125883307554298]]}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "data1 = [1, 2, 5, 9, 4, 27, 5, 'Memorial Day', True, 0, 0.0, 0.0, 65]\n",
    "\n",
    "data2 = [[1, 3, 10, 15, 4, 27, 7, 'None', False, 0, 2.0, 1.0, 80], \n",
    "         [1, 2, 5, 9, 4, 27, 5, 'Memorial Day', True, 0, 0.0, 0.0, 65]]\n",
    "\n",
    "result = aci_service.run(json.dumps(data1))\n",
    "print('Predictions for data1')\n",
    "print(result)\n",
    "\n",
    "result = aci_service.run(json.dumps(data2))\n",
    "print('Predictions for data2')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
